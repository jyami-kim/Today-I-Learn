# 5. 안정 해시 설계 부록

### 2. Consistent Hashing

https://tom-e-white.com/2007/11/consistent-hashing.html

캐시간 매우 균일하지 않은 개체 분포를 가질 수 있다. 

- 해결책 : 원에 있는 캐시 포인트의 복제본인 "가상 노드" 개념을 도입한다.
- 캐시 추가마다 해시링 안에 많은 가상 노드를 생성한다.



##### 코드 관련 설명

circle : hash값을 key로한 캐시이다.

ConsistentHash 객체가 생성되면 각 노드는 circle map을 생성한다. (numberOfReplicas 만큼). 각 replica의 위치는 노드이름+i (numerical suffix)로 해싱된 곳에 위치한다.

객체에 대한 노드를 찾을 때 : 해시 공간에 위치할것이므로, 해시 링에서 가까운 가상노드를 찾아야할 것 따라서 TreeMap의 tailMap() 을 사용했다.

```java
// class TreeMap<K,V>

SortedMap<K,V> tailMap(K fromKey) // Returns a view of the portion of this map whose keys are greater than or equal to fromKey.
```



### 3. Dynamo: Amazon's Highly Available Key-value Store

아마존 다이나모 데이터베이스(DynamoDB)의 파티셔닝 관련 컴포넌트

https://dl.acm.org/doi/10.1145/1294261.1294281



### 5. How Discord Scaled Elixir to 5,000,000 Concurrent Users

https://blog.discord.com/scaling-elixir-f9b8e1e7c29b

처음부터 디스코드는 Elixir의 얼리 어답터였다. Erlang VM 은 동시성이 높은 실시간 시스템을 위한 완벽한 후보였다.

> https://ko.wikipedia.org/wiki/Elixir
>
> Elixir : 얼랭(Erlang) 가상 머신(BEAM) 위에서 동작하는 함수형, 동시성 프로그래밍 언어이다
> 엘릭서는 얼랭이 보유하고 있는 분산 처리, 장애 내구성, 실시간, 무정지 애플리케이션 등의 특징을 공유한다.



디스코드는 대부분 pub/sub 으로 요약된다.

User는 WebSocket에 연결하고 GenServer라는 하나의 세션 프로세스를 시작한다.
GenServer : remote Erlang node들과 커뮤니케이션한다. (길드 프로세스를 담고있다.)

길드에 게시된 내용은 연결된 모든 세션에 퍼진다.

사용자가 온라인이면 > 길드에 연결되고 > 길드는 연결된 다른 모든 세션에 현재 상태를 게시한다. 



##### 문제

25명 미만의 그룹으로 만들때 위 방식이 훌륭했다. 그러나 사람들이 대규모 그룹을 위해 Discord를 사용하기 시작했다. 오버워치 서버는 30000명의 유저들이 들어가있다.

message queues에서 프로세스가 따라가지 못하는 현상을 발견함 . 따라서 이 현상이 발생하는 특정시간에는 수동으로 부하를 대처하면서 개입했다.



##### 문제 발견

Erlang 프로세스 간 메시지 전송이 저렴하지 않았다. (프로세스 스케줄링에 사용되는 Erlang의 하나의 work에 대한)절감 비용도 높았다.

하나의 send/2 호출에대해 사람이 인지할 수 있는 시간이 30μs ~ 70us > 즉 30000명이 있는 유저들이 있는 서버에서는 900ms ~ 2.1s

Erlang 프로세스는 하나의 쓰레드로 되어있었고, 작업을 병렬화하는 방법은 shard 였다.



##### 해결 방법 : Manifold의 탄생

Manifold는 remote node의 PID들에게 메세지를 보내는 work를 분리했다. 이것은 보내는 프로세스가 적어도 한번의 호출을 send/2 와 같은 remote nodes들의 수를 호출한다는 것이다. (? 대충 Remote node 만큼 호출한다는건가 적어도)  

Manifold는 이것을 먼저 remote noder로 PID를 그루핑하고, 각 노드를 Manifold.Partitioner로 보낸다. 이때 Partitioner는 PID를  consistently hash한다

````shell
:erlang.pash2/2  # core 수 만큼 그룹핑하고, 그들을 child worker로 보낸다.
````

? 근데 이해가 안됌ㅋㅋㅋㅋㅋ



##### 효과

- message를 보내는데 드는 CPU cost를 분배함
- 노드 사이 network traffic을 줄임.



##### 공유된 데이터들의 빠른 접근

Discord는 consistent hashing을 통해 분산 시스템에 도달했다.

이 방법을 사용하기위해서는 ring data strucutre가 필요했고, 이것들은 각각의 특정엔티티에 대한 노드를 사용했다.

따라서 라이브러리를 사용함 : https://github.com/chrismoos/hash-ring (Erlang 을 지원함. : C 코드로 되어있음.)

하지만 문제가 생김 : 처음엔 잘 작동했으나, 유저들이 많아짐에 따라서 이슈들이 발생하기 시작했다.

링 제어를 담당하는 Erlang 프로세스가 너무 바빠서 링에 대한 요청을 따라가지 못하고 전체 시스템에 과부하가 걸리게 되었다.



점차 문제가 발생했는데 해결방법 : 빠른 데이터 접근이 필요할 때는 ETS를 사용했다. ETS는 C로 구현된 mutable dictionary 이다. > 성능이 향상되었다. 그래도 여전히 get, search 로직이 느렸음 > vm 기능을 활용하는 모듈인 mochinglobal을 찾음. 엄청나게 속도가 향상되었다. (그러나 런타임에 링만큼 큰 데이터 구조로 모듈 빌드 비용이 1초가 소요함.) > FastBlobal이라는 새로운 모듈을 만들게되었음..



### 조대협님 Amazon Dynamo 계열 NoSQL

https://bcho.tistory.com/622

- Dynamo 계열의 NoSQL의 특징은 Ring Topology를 기본으로하고있다.
- N-Value (Quorum)은 노드의 복제본을 이야기한다. (일반적으로 3개의 복제본을 생성한다.)
- R-Value, W-Value는 성능과, 데이타 정합성간의 Trade-Off를 위한 값이다. : 데이타 복제는 실시간으로 이루어지지 않는다.
- R-Value : N-Value가 3일때, 세개의 노드 중에서 데이터를 몇개의 노드에서 읽어올지를 결정하는 것
- W-Value : 동시에 몇개의 Node에 Write를 할 것인가를 결정함.
- R-Value + W-Value > N-Value이면 Data Consistency가 보장된다.
- Masterless 아키텍처 : Ring을 구성하는 아무 Node에나 요청을 보내도 처리가 되고, 전체 설정 정보를 가지고 있는 마스터 노드나 Admin 노드가 없다.
- 가상노드 방식이 아닌 것 같다.



### 시류스 분산 웹캐시 개선 과정

https://tech.kakao.com/2017/10/23/wcache-1/



